variant: fcos
version: 1.6.0
passwd:
  users:
    - name: core
      ssh_authorized_keys_local:
        - ssh-key.pub

storage:
  disks:
  # This defines two partitions, each on its own disk. The disks are
  # identified by their WWN.
    - device: /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi1
      wipe_table: true
      partitions:
      - label: "container_data"
  
  filesystems:
    - path: /var/mnt/container_data
      device: /dev/disk/by-partlabel/container_data
      format: ext4
      label: container_data

  files:
    - path: /etc/hostname
      mode: 0644
      contents:
        inline: frigate-srv

    # I'm not sure exactly what this pager acheives but it seems to change the graphial console to
    # add a scroll bar
    - path: /etc/profile.d/systemd-pager.sh
      mode: 0644
      contents:
        inline: |
          # Tell systemd to not use a pager when printing information
          export SYSTEMD_PAGER=cat
    
    # Only reboot the server late on the weekend
    - path: /etc/zincati/config.d/55-updates-strategy.toml
      contents:
        inline: |
          [updates]
          strategy = "periodic"
          [[updates.periodic.window]]
          days = [ "Sat", "Sun" ]
          start_time = "00:30"
          length_minutes = 60

    # Use a static IP to interact with the cameras. Use .2 because the router is .1 on any vlans it connects to
    - path: /etc/NetworkManager/system-connections/static-ens19.nmconnection
      mode: 0600
      contents:
        inline: |
          [connection]
          id=static-ens19
          type=ethernet
          interface-name=ens19
          autoconnect=true

          [ipv4]
          method=manual
          addresses=192.168.254.2/24

          [ipv6]
          method=disabled

    # Add the repo for nvidia drivers
    # Using RHEL9 - apparently that's better and more stable for FCOS than plain fedora ones
    - path: /etc/yum.repos.d/nvidia.repo
      mode: 0644
      contents:
        inline: |
          [nvidia]
          name=NVIDIA CUDA for RHEL9
          baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64
          enabled=1
          gpgcheck=1
          gpgkey=https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/D42D0685.pub

    # Nvidia plugin to docker runtime
    - path: /etc/docker/daemon.json
      mode: 0644
      contents:
        inline: |
          {
            "default-runtime": "runc",
            "runtimes": {
              "nvidia": {
                "path": "nvidia-container-runtime",
                "runtimeArgs": []
              }
            }
          }

  directories:
    - path: /mnt/container_data/volumes
  
  links:
    - path: /etc/localtime
      target: ../usr/share/zoneinfo/Australia/Brisbane
    
    # Store docker volumes on separate (less ephemeral) disk
    - path: /var/lib/docker/volumes
      target: /mnt/container_data/volumes

systemd:
  units:
  # Auto login on both serial and VGA/virtual terminal
  - name: serial-getty@ttyS0.service
    dropins:
    - name: autologin-core.conf
      contents: |
        [Service]
        # Override Execstart in main unit
        ExecStart=
        # Add new Execstart with `-` prefix to ignore failure`
        ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM
  - name: getty@.service
    dropins:
    - name: autologin-core.conf
      contents: |
        [Service]
        # Override Execstart in main unit
        ExecStart=
        # Add new Execstart with `-` prefix to ignore failure`
        ExecStart=-/usr/sbin/agetty --autologin core --noclear %I $TERM

  # Add nvidia drivers
  - name: rpm-ostree-install-nvidia.service
    enabled: true
    contents: |-
      [Unit]
      Description=Install NVIDIA driver via rpm-ostree
      Wants=network-online.target
      After=network-online.target
      ConditionPathExists=!/var/lib/nvidia-driver-installed

      [Service]
      Type=oneshot
      ExecStart=/usr/bin/bash -c '
        set -euxo pipefail
        rpm-ostree install -y nvidia-driver nvidia-container-toolkit
        touch /etc/nvidia-driver-installed
      '
      RemainAfterExit=true

      [Install]
      WantedBy=multi-user.target

  # Second disk for docker container data
  # This concept copied from the unifi VM. I'm not sure why unifi uses bind mounts but this will
  # both allow for bind mounts and put volumes on the separate (less ephemeral) disk
  # Note: it's mounted at /var/mnt/container_data but only because /mnt points to /var/mnt,
  # uses of it should refer to /mnt/container_data.
  - name: var-mnt-container_data.mount
    enabled: true
    contents: |-
      [Unit]
      Description=Container data directory

      [Mount]
      What=/dev/disk/by-label/container_data
      Where=/var/mnt/container_data
      Type=auto

      [Install]
      WantedBy=multi-user.target
  - name: var-mnt-container_data.automount
    enabled: true
    contents: |-
      [Unit]
      Description=Container data automount

      [Automount]
      Where=/var/mnt/container_data
      [Install]
      WantedBy=multi-user.target

  - name: docker.portainer.service
    enabled: true
    contents: |-
      [Unit]
      Description=Portainer Admin Container
      After=docker.service
      Requires=docker.service var-mnt-container_data.mount

      [Service]
      Type=oneshot
      RemainAfterExit=yes
      TimeoutStartSec=0
      ExecStartPre=-/usr/bin/docker stop %n
      ExecStartPre=-/usr/bin/docker rm %n
      ExecStartPre=/usr/bin/docker pull portainer/portainer-ce:latest
      ExecStart=-/usr/bin/mkdir -p /mnt/container_data/portainer_data
      # Privileged mode is required for binding to local socket to work due to SELINUX (https://github.com/portainer/portainer/issues/849)
      ExecStart=/usr/bin/docker run --privileged=true -d --name %n --restart always \
        -p 8000:8000 -p 9443:9443 -p 9000:9000 \
        -v /var/run/docker.sock:/var/run/docker.sock \
        -v /mnt/container_data/portainer_data:/data \
        portainer/portainer-ce:latest
      ExecStop=/usr/bin/docker stop -t 15 %n

      [Install]
      WantedBy=multi-user.target
  - name: docker.qemu-agent.service
    enabled: true
    contents: |-
      [Unit]
      Description=QEMU Guest Agent Container
      After=docker.service
      Requires=docker.service var-mnt-container_data.mount

      [Service]
      Type=oneshot
      RemainAfterExit=yes
      TimeoutStartSec=0
      ExecStartPre=-/usr/bin/docker stop %n
      ExecStartPre=-/usr/bin/docker rm %n
      # Privileged mode is required for binding to local socket to work due to SELINUX (https://github.com/portainer/portainer/issues/849)
      ExecStart=/usr/bin/docker run --rm -d --name %n \
        -v /dev:/dev \
        -v /etc/os-release:/etc/os-release:ro \
        -v /var/run/dbus/system_bus_socket:/var/run/dbus/system_bus_socket \
        -v /sys/fs/cgroup:/sys/fs/cgroup \
        -v /sbin/shutdown:/sbin/shutdown \
        -v /bin/systemctl:/bin/systemctl \
        -v /usr/lib/systemd:/usr/lib/systemd \
        -v /lib64:/lib64 \
        --privileged \
        --uts=host \
        --net=host \
        docker.io/danskadra/qemu-ga -v
      ExecStop=/usr/bin/docker stop -t 15 %n

      [Install]
      WantedBy=multi-user.target
  # - name: docker.watchtower.service
  #   enabled: true
  #   contents: |-
  #     [Unit]
  #     Description=Watchtower Container
  #     After=docker.service
  #     Requires=docker.service

  #     [Service]
  #     Type=oneshot
  #     RemainAfterExit=yes
  #     TimeoutStartSec=0
  #     ExecStartPre=-/usr/bin/docker stop %n
  #     ExecStartPre=-/usr/bin/docker rm %n
  #     ExecStartPre=/usr/bin/docker pull containrrr/watchtower
  #     ExecStart=/usr/bin/docker run --privileged -d --name %n -v /var/run/docker.sock:/var/run/docker.sock:z containrrr/watchtower
  #     ExecStop=/usr/bin/docker stop -t 15 %n

  #     [Install]
  #     WantedBy=multi-user.target
